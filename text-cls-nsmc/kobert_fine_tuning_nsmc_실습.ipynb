{"cells":[{"cell_type":"code","source":["!git clone https://github.com/hanakim120/NLU_BERT.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seTon9OzrpDc","executionInfo":{"status":"ok","timestamp":1688722791659,"user_tz":-540,"elapsed":5295,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}},"outputId":"b393e5ec-ac16-48d0-ad9f-6707fb176716"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'NLU_BERT'...\n","remote: Enumerating objects: 59, done.\u001b[K\n","remote: Counting objects: 100% (59/59), done.\u001b[K\n","remote: Compressing objects: 100% (55/55), done.\u001b[K\n","remote: Total 59 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (59/59), 10.50 MiB | 3.21 MiB/s, done.\n"]}]},{"cell_type":"code","source":["!pip install numpy==1.23"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLhDKmm1sHS_","executionInfo":{"status":"ok","timestamp":1688722916868,"user_tz":-540,"elapsed":7248,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}},"outputId":"76055ab8-6146-4399-a9ca-57a216ed0238"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.23 in /usr/local/lib/python3.10/dist-packages (1.23.0)\n"]}]},{"cell_type":"code","source":["# install\n","!pip install datasets transformers sentencepiece accelerate -U evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG-QiRoToywG","executionInfo":{"status":"ok","timestamp":1688722944637,"user_tz":-540,"elapsed":22233,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}},"outputId":"3956d1c8-daf5-453e-adf6-a50e0a83b078"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n","Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n","  Downloading huggingface_hub-0.16.3-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate, accelerate\n","Successfully installed accelerate-0.20.3 datasets-2.13.1 dill-0.3.6 evaluate-0.4.0 huggingface-hub-0.16.3 multiprocess-0.70.14 responses-0.18.0 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0\n"]}]},{"cell_type":"code","source":["import transformers\n","transformers.logging.set_verbosity_error()"],"metadata":{"id":"_Us6Q_mhr5O5","executionInfo":{"status":"ok","timestamp":1688722956050,"user_tz":-540,"elapsed":1777,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(action='ignore')"],"metadata":{"id":"M2p8cAPSr5L7","executionInfo":{"status":"ok","timestamp":1688722957354,"user_tz":-540,"elapsed":291,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""],"metadata":{"id":"8aI1lU6Nr5Jg","executionInfo":{"status":"ok","timestamp":1688722958618,"user_tz":-540,"elapsed":2,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["BASE_DIR = '/content/NLU_BERT/text-cls-nsmc'\n","os.chdir(BASE_DIR)\n","OUTPUT_DIR = os.path.join(BASE_DIR, 'checkpoint')\n","os.makedirs(OUTPUT_DIR, exist_ok=True)"],"metadata":{"id":"LCTyf3VHsd6s","executionInfo":{"status":"ok","timestamp":1688722959667,"user_tz":-540,"elapsed":2,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","DEVICE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yPd8vT8uKfY","executionInfo":{"status":"ok","timestamp":1688722984533,"user_tz":-540,"elapsed":7363,"user":{"displayName":"Hana Kim","userId":"00484702240680103745"}},"outputId":"ba058dd0-418a-4bb1-b8be-70f69ac981a6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["- 훈련시키기"],"metadata":{"id":"IjMIq0XvvueM"}},{"cell_type":"code","source":["!python kobert_ft_hftrainer.py --model_fn ./checkpoint/best_model.pth --train_fn ./data/nsmc_train.tsv --n_epoch 3"],"metadata":{"id":"aZmMSFRToZ8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def define_argparser():\n","    # command line argument를 받기 위한 함수\n","    p = argparse.ArgumentParser()\n","\n","    p.add_argument('--model_fn', required=True) # 저장될 모델 파일 이름\n","    p.add_argument('--train_fn', required=True) # 학습에 사용될 파일 이름\n","    p.add_argument('--pretrained_model_name', type=str, default='beomi/kcbert-base') # 사전 학습된 모델 이름\n","    #  pretrained_model_name 예시:\n","    # - beomi/kcbert-base\n","    # - beomi/kcbert-large\n","    p.add_argument('--valid_ratio', type=float, default=.2) # valid set 비율\n","    p.add_argument('--batch_size_per_device', type=int, default=32) # device 당 batch size\n","    p.add_argument('--n_epochs', type=int, default=5) # epoch 수\n","    p.add_argument('--warmup_ratio', type=float, default=.2) # warmup 비율\n","    p.add_argument('--max_length', type=int, default=100) # 최대 길이\n","\n","    config = p.parse_args() # argument를 config에 저장\n","\n","    return config"],"metadata":{"id":"W5sfdorcnkKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bV08OoxEjaHE"},"outputs":[],"source":["# 패키지 설치\n","import argparse # command line argument를 받기 위한 package\n","import random # random seed를 고정하기 위한 package\n","from sklearn.metrics import accuracy_score # 정확도 계산을 위한 package\n","import torch # pytorch package\n","from transformers import BertTokenizerFast # tokenizer package\n","from transformers import BertForSequenceClassification, AlbertForSequenceClassification # model package\n","from transformers import Trainer  # training을 위한 package\n","from transformers import TrainingArguments # training argument를 설정하기 위한 package\n","from transformers import DataCollatorWithPadding # padding을 위한 package\n","\n","from utils.utils import read_text # 데이터 읽어오는 함수\n","from utils.bert_dataset import TextClassificationDataset # dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHFpnH7bjaHF"},"outputs":[],"source":["def define_argparser():\n","    # command line argument를 받기 위한 함수\n","    p = argparse.ArgumentParser()\n","\n","    p.add_argument('--model_fn', required=True) # 저장될 모델 파일 이름\n","    p.add_argument('--train_fn', required=True) # 학습에 사용될 파일 이름\n","    p.add_argument('--pretrained_model_name', type=str, default='beomi/kcbert-base') # 사전 학습된 모델 이름\n","    #  pretrained_model_name 예시:\n","    # - beomi/kcbert-base\n","    # - beomi/kcbert-large\n","    p.add_argument('--valid_ratio', type=float, default=.2) # valid set 비율\n","    p.add_argument('--batch_size_per_device', type=int, default=32) # device 당 batch size\n","    p.add_argument('--n_epochs', type=int, default=5) # epoch 수\n","    p.add_argument('--warmup_ratio', type=float, default=.2) # warmup 비율\n","    p.add_argument('--max_length', type=int, default=100) # 최대 길이\n","\n","    config = p.parse_args() # argument를 config에 저장\n","\n","    return config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnsX2EbDjaHF"},"outputs":[],"source":["def get_datasets(fn, tokenizer, valid_ratio=.2):\n","     # 데이터 파일을 읽어서 labels와 texts list 받기\n","    labels, texts = read_text(fn)\n","\n","    # label을 index로 바꾸기 위한 과정\n","    unique_labels = list(set(labels)) # 중복 제거\n","    label_to_index = {} # label을 index로 바꾸기 위한 dict\n","    index_to_label = {} # index를 label로 바꾸기 위한 dict\n","\n","    for i, label in enumerate(unique_labels):\n","        label_to_index[label] = i\n","        index_to_label[i] = label\n","\n","    # labels 리스트의 각 요소를 label_to_index 딕셔너리에서 해당하는 값으로 변환하여 새로운 리스트를 생성\n","    labels = list(map(label_to_index.get, labels))\n","\n","    # texts와 labels를 zip으로 묶고, random.shuffle로 섞기\n","    shuffled = list(zip(texts, labels))\n","    random.shuffle(shuffled)\n","\n","    # 섞인 데이터를 다시 풀어서 texts와 labels로 나누기\n","    texts = [e[0] for e in shuffled]\n","    labels = [e[1] for e in shuffled]\n","\n","    # valid set의 비율에 따라서 index 설정\n","    idx = int(len(texts) * (1 - valid_ratio))\n","\n","    # train set과 valid set으로 나누기\n","    train_dataset = TextClassificationDataset(texts[:idx], labels[:idx], tokenizer) # 앞에서부터 idx까지\n","    valid_dataset = TextClassificationDataset(texts[idx:], labels[idx:], tokenizer) # idx부터 끝까지\n","\n","\n","    return train_dataset, valid_dataset, index_to_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eas9yuLxjaHG"},"outputs":[],"source":["def main(config):\n","    # pretrain된 tokenizer 불러오기\n","    tokenizer = BertTokenizerFast.from_pretrained(config.pretrained_model_name)\n","\n","    # datasets과 index_to_label 받기\n","    train_dataset, valid_dataset, index_to_label = get_datasets(\n","        config.train_fn,\n","         tokenizer,\n","        valid_ratio=config.valid_ratio,\n","    )\n","\n","    print(\n","        '## train ## =', len(train_dataset),\n","        '## valid ## =', len(valid_dataset),\n","    )\n","\n","    total_batch_size = config.batch_size_per_device * torch.cuda.device_count() # 전체 batch size\n","    n_total_iterations = int(len(train_dataset) / total_batch_size * config.n_epochs) # 전체 iteration 수\n","    n_warmup_steps = int(n_total_iterations * config.warmup_ratio) # warmup step 수\n","\n","    print(\n","        '## total_iters ## =', n_total_iterations,\n","        '## warmup_iters ## =', n_warmup_steps,\n","    )\n","\n","    # pretrained_model_name에 따라서 model loader를 다르게 설정\n","    model_loader = BertForSequenceClassification\n","    # pretrained model 불러오기\n","    model = model_loader.from_pretrained(\n","        config.pretrained_model_name, # pretrained model 이름\n","        num_labels=len(index_to_label) # output label 개수\n","    )\n","\n","    training_args = TrainingArguments(\n","        output_dir='./.checkpoints', # checkpoint 저장 경로\n","        num_train_epochs=config.n_epochs, # epoch 수\n","        per_device_train_batch_size=config.batch_size_per_device, # device 당 train batch size\n","        per_device_eval_batch_size=config.batch_size_per_device, # device 당 eval batch size\n","        warmup_steps=n_warmup_steps, # warmup step 수\n","        weight_decay=0.01, # weight decay\n","        fp16=True, # AMP 사용 여부\n","        evaluation_strategy='epoch', # epoch 단위로 eval\n","        save_strategy = 'epoch', # epoch 단위로 저장\n","        logging_steps=n_total_iterations // 100, # logging step 수\n","        load_best_model_at_end=True, # best model 불러오기 여부\n","    )\n","\n","    def compute_metrics(pred):\n","        # prediction과 label을 받아서 accuracy 계산\n","        labels = pred.label_ids # np.ndarray\n","        # pred.predictions : label 예측값 (np.ndarray)\n","        preds = pred.predictions.argmax(-1) # 가장 높은 확률을 가진 label 예측값의 index\n","\n","        return {\n","            'accuracy': accuracy_score(labels, preds)\n","        }\n","\n","    # NLP는 미니 배치 안에 각 문장들의 길이가 다르기 때문에 padding을 해줘야 함\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # 동적 padding을 위한 data collator\n","\n","    # trainer 설정\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        data_collator=data_collator, # 배치마다 패딩\n","        train_dataset=train_dataset,\n","        eval_dataset=valid_dataset,\n","        compute_metrics=compute_metrics, # metric 계산\n","    )\n","    # train 시작\n","    trainer.train()\n","\n","    torch.save({\n","        'bert': trainer.model.state_dict(), # best model의 weight 저장\n","        'config': config,\n","        'vocab': None,\n","        'classes': index_to_label,\n","        'tokenizer': tokenizer,\n","    }, config.model_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtipGsBmjaHG"},"outputs":[],"source":["if __name__ == '__main__':\n","    config = define_argparser()\n","    main(config)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}